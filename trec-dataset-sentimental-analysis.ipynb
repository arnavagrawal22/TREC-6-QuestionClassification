{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:26:58.148916Z","iopub.status.busy":"2022-02-12T14:26:58.148586Z","iopub.status.idle":"2022-02-12T14:26:58.868401Z","shell.execute_reply":"2022-02-12T14:26:58.867696Z","shell.execute_reply.started":"2022-02-12T14:26:58.148874Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchtext.legacy import data\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize = 'spacy',\n","                  tokenizer_language = 'en_core_web_sm',\n","                  include_lengths = True) #to make sure we get actual length of batch in return value\n","\n","LABEL = data.LabelField(dtype = torch.int64)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:26:59.95394Z","iopub.status.busy":"2022-02-12T14:26:59.953634Z","iopub.status.idle":"2022-02-12T14:27:00.806577Z","shell.execute_reply":"2022-02-12T14:27:00.805848Z","shell.execute_reply.started":"2022-02-12T14:26:59.953901Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["downloading train_5500.label\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 336k/336k [00:01<00:00, 199kB/s]  \n"]},{"name":"stdout","output_type":"stream","text":["downloading TREC_10.label\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 23.4k/23.4k [00:00<00:00, 74.8kB/s]\n"]}],"source":["from torchtext.legacy import datasets\n","\n","train_dataset, test_dataset = datasets.TREC.splits(TEXT, LABEL)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:27:00.80915Z","iopub.status.busy":"2022-02-12T14:27:00.808241Z","iopub.status.idle":"2022-02-12T14:27:00.816081Z","shell.execute_reply":"2022-02-12T14:27:00.814686Z","shell.execute_reply.started":"2022-02-12T14:27:00.809101Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5452\n","250\n","250\n"]}],"source":["#we need to split the test into val and test data\n","val_dataset,test_dataset = test_dataset.split(0.5)\n","print(len(train_dataset))\n","print(len(test_dataset))\n","print(len(val_dataset))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:27:00.817876Z","iopub.status.busy":"2022-02-12T14:27:00.817529Z","iopub.status.idle":"2022-02-12T14:27:01.96347Z","shell.execute_reply":"2022-02-12T14:27:01.962415Z","shell.execute_reply.started":"2022-02-12T14:27:00.817833Z"},"trusted":true},"outputs":[],"source":["MAX_VOCAB_SIZE = 15000\n","\n","TEXT.build_vocab(train_dataset, \n","                 max_size = MAX_VOCAB_SIZE)\n","                 #needed if you are training the model, as model trained on Kaggle, commenting it\n","                #  vectors = \"glove.6B.100d\", #using pretrained embeddings\n","                #  unk_init = torch.Tensor.normal_) #initializing all vocab, but not in pretrained, to random values\n","\n","LABEL.build_vocab(train_dataset)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:27:01.966829Z","iopub.status.busy":"2022-02-12T14:27:01.966444Z","iopub.status.idle":"2022-02-12T14:27:01.973764Z","shell.execute_reply":"2022-02-12T14:27:01.97239Z","shell.execute_reply.started":"2022-02-12T14:27:01.966781Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n"]}],"source":["print(LABEL.vocab.itos[:])\n","# HUM for questions about humans\n","# ENTY for questions about entities\n","# DESC for questions asking you for a description\n","# NUM for questions where the answer is numerical\n","# LOC for questions where the answer is a location\n","# ABBR for questions asking about abbreviations"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:27:01.976122Z","iopub.status.busy":"2022-02-12T14:27:01.97579Z","iopub.status.idle":"2022-02-12T14:27:01.987712Z","shell.execute_reply":"2022-02-12T14:27:01.986866Z","shell.execute_reply.started":"2022-02-12T14:27:01.976078Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_dataset, val_dataset, test_dataset), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True, #for using packed, we need each batch sorted by length\n","    device = device)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:27:01.989881Z","iopub.status.busy":"2022-02-12T14:27:01.98954Z","iopub.status.idle":"2022-02-12T14:27:02.003185Z","shell.execute_reply":"2022-02-12T14:27:02.002044Z","shell.execute_reply.started":"2022-02-12T14:27:01.989838Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx) #PAD INDEX as we \n","        #don't want to learn the embeddings for the paddings\n","        \n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        \n","        self.fc = nn.Linear(hidden_dim * 2, output_dim) #no need for relu or anything as loss will cover it\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text, text_lengths): #we need to give text lenghths in each pass, as we are using packed \n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))\n","        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        #pack sequence\n","        # lengths need to be on CPU!\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n","        #to cpu is in documentation, text_lengths we will get from iterator, and its actual lenght of each sen\n","        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded) #LSTM returns 3 things\n","        \n","        #unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors as they aren't even trained\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) #last two hidden are concat\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:27:37.660438Z","iopub.status.busy":"2022-02-12T14:27:37.660154Z","iopub.status.idle":"2022-02-12T14:27:37.697946Z","shell.execute_reply":"2022-02-12T14:27:37.696975Z","shell.execute_reply.started":"2022-02-12T14:27:37.660408Z"},"trusted":true},"outputs":[],"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 6\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:27:38.494673Z","iopub.status.busy":"2022-02-12T14:27:38.494374Z","iopub.status.idle":"2022-02-12T14:27:38.502254Z","shell.execute_reply":"2022-02-12T14:27:38.501313Z","shell.execute_reply.started":"2022-02-12T14:27:38.494637Z"},"trusted":true},"outputs":[],"source":["# putting pretrained embeddings and initializing embedding layer\n","pretrained_embeddings = TEXT.vocab.vectors\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","print(pretrained_embeddings.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:27:38.930304Z","iopub.status.busy":"2022-02-12T14:27:38.930007Z","iopub.status.idle":"2022-02-12T14:27:38.942409Z","shell.execute_reply":"2022-02-12T14:27:38.94129Z","shell.execute_reply.started":"2022-02-12T14:27:38.930269Z"},"trusted":true},"outputs":[],"source":["#changing <pad> and <unk> embeddings to zero(Not compolsary)\n","model.embedding.weight.data[0] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[1] = torch.zeros(EMBEDDING_DIM)\n","print(model.embedding.weight.data)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:29:10.577801Z","iopub.status.busy":"2022-02-12T14:29:10.577065Z","iopub.status.idle":"2022-02-12T14:33:02.655346Z","shell.execute_reply":"2022-02-12T14:33:02.654449Z","shell.execute_reply.started":"2022-02-12T14:29:10.577746Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n","criterion = nn.CrossEntropyLoss()\n","model = model.to(device)\n","def categorical_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    top_pred = preds.argmax(1, keepdim = True)\n","    correct = top_pred.eq(y.view_as(top_pred)).sum()\n","    acc = correct.float() / y.shape[0]\n","    return acc\n","\n","def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        \n","        text,text_len = batch.text\n","        predictions = model(text,text_len).squeeze(1)\n","        \n","        loss = criterion(predictions, batch.label)\n","        \n","        acc = categorical_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            text,text_len = batch.text\n","            predictions = model(text,text_len).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = categorical_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","# N_EPOCHS = 15\n","# from tqdm import tqdm \n","\n","# for epoch in tqdm(range(N_EPOCHS)):\n","\n","#     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","#     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)    \n","#     print(f'Epoch: {epoch+1:02}')\n","#     print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","#     if (epoch%5 == 0):\n","#         torch.save(model.state_dict(), 'trec-5-epoch-{}.pt') "]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:33:20.238945Z","iopub.status.busy":"2022-02-12T14:33:20.238029Z","iopub.status.idle":"2022-02-12T14:33:20.55421Z","shell.execute_reply":"2022-02-12T14:33:20.553499Z","shell.execute_reply.started":"2022-02-12T14:33:20.238895Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Loaded Successfully\n","0.8830818980932236\n","0.9108297377824783\n"]}],"source":["model.load_state_dict(torch.load('trec-5-epoch.pt', map_location=device))\n","print(\"Model Loaded Successfully\")\n","valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","train_loss, train_acc = evaluate(model, test_iterator, criterion)\n","print(valid_acc)\n","print(train_acc)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:33:32.303712Z","iopub.status.busy":"2022-02-12T14:33:32.302799Z","iopub.status.idle":"2022-02-12T14:33:33.032225Z","shell.execute_reply":"2022-02-12T14:33:33.031192Z","shell.execute_reply.started":"2022-02-12T14:33:32.303657Z"},"trusted":true},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","import numpy as np\n","def predict_class(model, sentence, min_len = 4):\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    if len(tokenized) < min_len:\n","        tokenized += ['<pad>'] * (min_len - len(tokenized))\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    length = [len(indexed)]\n","    length_tensor = torch.LongTensor(length)\n","    preds = model(tensor,length_tensor)\n","    max_preds = preds.argmax(dim = 1)\n","    return max_preds.item()"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-02-12T14:33:55.548971Z","iopub.status.busy":"2022-02-12T14:33:55.548629Z","iopub.status.idle":"2022-02-12T14:33:55.617463Z","shell.execute_reply":"2022-02-12T14:33:55.616451Z","shell.execute_reply.started":"2022-02-12T14:33:55.548935Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ENTY for questions about entities\n","NUM for questions where the answer is numerical\n","LOC for questions where the answer is a location\n","ABBR for questions asking about abbreviations\n"]}],"source":["classes = [\"HUM for questions about humans\",\n","\"ENTY for questions about entities\",\n","\"DESC for questions asking you for a description\",\n","\"NUM for questions where the answer is numerical\",\n","\"LOC for questions where the answer is a location\",\n","\"ABBR for questions asking about abbreviations\"]\n","\n","print(classes[predict_class(model,\"Who was Galileo ?\")])\n","print(classes[predict_class(model,\"How old am I?\")])\n","print(classes[predict_class(model,\"What continent is Bulgaria in?\")])\n","print(classes[predict_class(model,\"What does NLP stand for?\")])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
